\chapter{Evaluation}
This chapter aims to assess the effectiveness of the debugger. Speed is
measured to evaluate whether the debugger has any performance impact when
debugging standard and computative demanding programs. We also evaluate its
feature richness and ease of use, comparing it to state-of-the-art debuggers
like the GDB.

\section{The Development Process}
The development of the thesis was done in a GitHub repository. The power of
Github was leveraged not only for keeping history but also for recording
issues, planning the development, or ensuring that the repository is in a
consistent and working state via GitHub actions, which runs tests before a pull
request is accepted. The actions run on two operating systems, Ubuntu and
MacOS, ensuring the project works on both.

The project itself contains numerous tests. The code is first tested via many
unit tests using the \texttt{GoogleTest}~\cite{gtest} framework. The unit
tests cover almost all parts of the code. It also has integration tests for T86
CLI and the debugger CLI. Another student created a TinyC to T86 compiler as
part of his thesis~\cite{martintinyc}. He was kind enough to send the
implementation to us so that we could generate various tests more easily. As a
result, most of the integration tests were generated by the said compiler. The
integration tests for the T86 CLI run the program and check if its output is
the same as the expected one. The debugger CLI is also checked against expected
output. Its input, however, is not only the file that will be debugged, but
also the series of commands that will be executed. The tests can be found
in the thesis attachment on path \texttt{impl/src/dbg-cli/tests}.

\section{Ease of Use and User Testing}
We followed the interface of the GDB closely so that the users were familiar
with the debugger before even running it. However, we choose to diverge on some
of the features. For example, the GDB uses the \texttt{stepi} command for
single stepping. This form has a common prefix with the \texttt{step} command.
We, however, expect our users to use assembly-level debugging more often than
source-level debugging, so we choose the \texttt{istep} command form instead.
The two commands have no shared prefix, so typing \texttt{is} is enough.

Also, the program is run via \texttt{run}, this starts the VM, but the program
is paused. In GDB, one has to use the \texttt{start} command to get equivalent
behavior, \texttt{run} runs the program without stopping at the beginning. When
we did a brief user testing, it was not very clear to the user. However,
renaming the command \texttt{run} to \texttt{start} would cause it to have the
same prefix as \texttt{step}. Considering this, we've decided to leave the
command as \texttt{run}.

There are other minor things; most of them come from the fact that we
prioritize assembly-level debugging, whereas GDB focuses more on the source
level. We provide a short list of examples of commands that have different
syntax in GDB and in our debugger in the thesis attachment:
\texttt{impl/docs/debugging.md}.

One student already tested the debugger and said the experience was quite
pleasant. There were a few minor things that he did not like and offered
solutions for (for example, the usage message can only be displayed after the
debuggee program is executed, meaning the \texttt{run} command must be invoked
first), most of which we took to heart and corrected. Additionally, the student
that provided us with the TinyC compiler used the debugger to debug the code
his compiler generated. The NI-GEN students are also using the repository,
although no official feedback was collected because it is too early in the
semester

\section{Performance}\label{section:benchmark}
In this section, we will measure how much can the performance suffer if the
program is being debugged. We will also look which debugging features hurt
performance most. The benchmarking will be performed on two programs: a
quicksort~\cite{quicksort} algorithm and a naive prime number checker. Both of
these programs were generated by the previously mentioned TinyC to T86
compiler~\cite{martintinyc}. The compiler performed no optimizations. The code for
benchmarking can be found in the thesis attachment on path
\texttt{impl/src/benchmarks}.

We scaled the data the programs work with so that their runtime without the
debugger is similar. The main difference is that the prime number checker is
loop based, while the quicksort one uses recursion. This will make a difference
since some of the algorithms are making decisions based on the \texttt{CALL}
instruction, like step over.

% Intel Core i5 8265U Whiskey Lake
% 4 cores, 1.6GHz, 3.9GHz under Core Boost
% 6 MB cache
% 8 GB RAM

We will measure the speed on the following cases:
\begin{enumerate}
    \item Run the T86 Virtual machine without the debugger.
    \item Connect the debugger and let the program continue.
    \item Connect the debugger and set breakpoint at hot spot of the program.
        For quicksort, this will be the main recursive function. For primes, it
        will be the body of the loop. When the breakpoint is hit the program is
        continued, no further action is taken.
    \item Same as before, but at every breakpoint, hit read a $100$ cells of
        memory and the \texttt{IP} register.
    \item  Step over a computationally expensive function. For quicksort, it is
        the \texttt{quicksort} function; for primes, it is the \verb|is_prime|
        function.
    \item Set a breakpoint in the most expensive function and run the program.
        Remove the breakpoint on the first hit and step out of the function.
        The expensive functions are the same as in the previous test.
\end{enumerate}

\begin{table}[]
\centering
\begin{tabular}{||c c c||}
\hline
Test Case & Quicksort - Time & Prime numbers - Time \\
\hline\hline
1. & $7.91$            & $10.51$  \\
2. & $8.89$            & $12.33$  \\
3. & $8.73$            & $30.32$  \\
4. & $9.98$             & $31.85$  \\
5. & $8.91$            & $11.53$  \\
6. & $8.36$            & $130.81$ \\
\hline
\end{tabular}
\caption{Performance comparison when using various features of the debugger.
    Each case was run five times, and the average was taken. The time is in
    seconds.}
\label{table:benchmark}
\end{table}

The results are in table~\ref{table:benchmark}. It is apparent that just having
the debugger connected introduces a minor slowdown. The breakpoints, however,
do cause a slowdown. In the quicksort case, the program had $641$ breakpoint
hits, while in the prime numbers, it had $101265$ hits. With this amount  of
breakpoints, the difference is negligible, because such amount of breakpoint
hits will very rarely be experienced. Additional reading of memory and
registers did not cause any significant slowdown.

The step-over result is not very surprising. It puts a breakpoint after the
call and runs the program, so the result should be roughly the same as case
number one. Step out is stepping over until a return is encountered. This works
well in the recursive function because it skips a large part of the program.
However, in the prime case, which is implemented via a loop, it severely slows
down the program because it essentially single steps through the entire
function. This is something that could be improved in the future because the
presented case here might happen in production usage.

Besides that, most debugger features did not cause significant slowdowns, so
the debugger is suitable for use.
