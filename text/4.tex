\chapter{Evaluation}
This chapter aims to assess the effectiveness of the debugger. Speed is
measured to evaluate whether the debugger has any performance impact when
debugging standard and computative demanding programs. We also evaluate its
feature richness and ease of use, comparing it to state-of-the-art debuggers
like the GDB.

\section{The Development Process}
The development of the thesis was done in a GitHub repository. The power of
Github was leveraged not only for keeping history but also for recording
issues, planning the development, or ensuring that the repository is in a
consistent and working state via GitHub actions, which runs tests before a pull
request is accepted. The actions run on two operating systems, Ubuntu and
MacOS, ensuring the project works on both.

The project itself contains many tests. The code is first tested via many unit
tests using the \texttt{GoogleTest}\todo{citace?} framework. The unit tests
cover almost all parts of the code. It also has integration tests for T86 CLI
and the debugger CLI. Another student\todo{citace} created a TinyC to T86
compiler as part of his thesis. He was kind enough to send the implementation
to us so that we could generate various tests more easily. As a result, most of
the integration tests were generated by the said compiler. The integration
tests for the T86 CLI run the program and check if its output is the same as
the expected one. The debugger CLI is also checked against expected output.
Its input, however, is not only the file that will be debugged, but also the
series of commands that will be executed\todo{path to tests?}.

\section{Usage and User Testing}
We followed the interface of the GDB closely so that the users were familiar
with the debugger before even running it. However, we choose to diverge on some
of the features. For example, the GDB uses the \texttt{stepi} command for
single stepping. This form has a common prefix with the \texttt{step} command.
We, however, expect our users to use assembly-level debugging more often than
source-level debugging, so we choose the \texttt{istep} command form instead.
The two commands have no shared prefix, so typing \texttt{is} is enough.

Also, the program is run via \texttt{run}, this starts the VM, but the program
is paused. In GDB, one has to use the \texttt{start} command to get equivalent
behavior, \texttt{run} runs the program without stopping at the beginning. When
we did a brief user testing, it was not very clear to the user. However,
renaming the command \texttt{run} to \texttt{start} would cause it to have the
same prefix as \texttt{step}. Considering this, we've decided to leave the
command as \texttt{run}.

There are other minor things; most of them come from the fact that we
prioritize assembly-level debugging, whereas GDB focuses more on the source
level. We provide a short list of examples of commands that have different
syntax in GDB and in our debugger in the thesis attachment:
\texttt{impl/docs/debugging.md}.

One student already tested the debugger and said the experience was quite
pleasant. There were a few minor things that he did not like and offered
solutions for (for example, the usage message can only be displayed after the
debuggee program is executed, meaning the \texttt{run} command must be invoked
first), most of which we took to heart and corrected. Additionally, the student
that provided us with the TinyC compiler used the debugger to debug the code
his compiler generated. The NI-GEN students are also using the repository,
although no official feedback was collected because it is too early in the
semester

\section{Performance}\label{section:benchmark}
The performance of the debugged program can suffer. In this section, we show
which debugging features hurt performance the most and if there is any way
around it. The tests were done on two programs: a quicksort~\cite{quicksort}
algorithm and a naive prime number checker. The sources for these programs are
in the thesis attachments under path \texttt{benchmark/}. The difference
between those two programs is that the quicksort is recursive, whereas the
primes are implemented via a loop. The runtime of those two programs on the
virtual machine alone is similar.

\begin{table}[]
\centering
\begin{tabular}{||c c c||}
\hline
Test Case & Quicksort - Time & Prime numbers - Time \\
\hline\hline
1. & $9.96$            & $11.68$  \\
2. & $9.32$            & $12.33$  \\
3. & $10.9$            & $58.32$  \\
4. & $10.9$            & $72.85$  \\
5. & $9.73$            & $10.53$  \\
6. & $9.65$            & $230.81$ \\
\hline
\end{tabular}
\caption{Performance comparison when using various features of the debugger.
The Quicksort had $674$ breakpoints hits, while prime numbers had $101266$ hits.
    Each case was run five times and average was taken. The time is in seconds.}
\label{table:benchmark}
\end{table}

% Intel Core i5 8265U Whiskey Lake
% 4 cores, 1.6GHz, 3.9GHz under Core Boost
% 6 MB cache
% 8 GB RAM

We will measure the speed on the following cases:
\begin{enumerate}
    \item Run the VM without the debugger.
    \item Connect the debugger and immediately invoke the \texttt{continue} command.
    \item Connect the debugger and set breakpoint at hot spot of the program.
        For quicksort, this will be the main recursive function. For primes,
        it will be the body of the loop. When the breakpoint is hit invoke
        the \texttt{continue} command.
    \item Same as before, but at every breakpoint, hit read a $100$ cells of
        memory and the \texttt{IP} register.
    \item  Step over a computationally expensive function. For quicksort, it is
        the \texttt{quicksort} function; for primes, it is the \verb|is_prime|
        function.
    \item Set a breakpoint in the most expensive function and run the program.
        Remove the breakpoint on the first hit and step out of the function.
        The expensive functions are the same as in the previous test.
\end{enumerate}

The results are in table \ref{table:benchmark}. It is apparent that just having
the debugger connected introduces almost no slowdown. In the quicksort case,
the program is even faster. This is probably due to measurement errors. The
breakpoints, however, do cause a slowdown. In the quicksort, the program had
$674$ breakpoint hits, while in the prime numbers, it had $101266$. The
quicksort version is $1.1$ times slower, which is negligible. However, in the
prime numbers test performance takes a severe hit with being at least $5$ times
slower. This shouldn't come as a surprise, as there is an enormous number of
breakpoint hits. The communication between the debugger and the debuggee starts
to slow down the program. Given the circumstances, this result is still
satisfactory. Reading memory and registers together with the breakpoints
introduces minimal slowdown.

The step-over result is not very surprising. It puts a breakpoint after the
call and runs the program, so the result should be roughly the same as case
number one. Step out is stepping over until a return is encountered. This works
well in the recursive function because it skips a large part of the program.
But in the prime case, which is implemented via a loop, it severely slows down
the program because it essentially single steps through the entire thing.

The debugger is more than suited for usage on regular programs. In the case of
very computationally intensive programs, certain debugger features will start
to struggle. On the other hand, a hundred thousand breakpoint hits is an
unpractically large number. We consider the quicksort example as a peak of what
the students will have to debug, and in that example, the debugger had almost
no slowdowns. Still, it is something to keep in mind while using the debugger,
and there is a potential for improvements in the future.
