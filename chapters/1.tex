\chapter{Debugging support}

\begin{quote}
  \textit{Debugging is twice as hard as writing the code in the first place. Therefore, if you write the code as cleverly as possible, you are, by definition, not smart enough to debug it.}\begin{flushright}
    \tiny{Brian W. Kernighan}
  \end{flushright}
\end{quote}

We already mentioned in the first chapter that to debug programs written in high level programming languages, we need the compiler to emit debugging information.
But debugging support must also be provided by operating system (if there is one) and by processor itself.
However, we can still debug programs in machine code. We briefly mentioned this in chapter 1 \todo{ref},
however, we won't see source code, but only assembly. This can still be useful, for example
for reverse engineering. And, as was already hinted, source level debugging is built upon
assembly debugging. In this chapter, we will describe on which levels must assembly level debugging
be supported. Additionaly, we'll discuss how are compilers and debuggers able to allows us 
to debug source code althrough the programs are still machine code programs. 

\section{Support on CPU level}
In first chapter it was said that CPU can only execute machine code which is made of instructions
and that it has certain registers. Which instructions and registers the CPU has can differ from CPU to CPU.
This is specified by \textit{Instruction Set Architecture} (ISA) \cite{aps-isa}. It is an abstract interface between
the hardware and lowest level software (machine code). It contains all information needed to write a program in machine
code. In general, ISA specifies following:
\begin{itemize}
    \item Set of machine code instructions - Specifies instructions the ISA has and what operands each instruction has.
    \item Register set - Which registers the ISA has\footnote{Strictly speaking ISA doesn't have to use registers. It's possible to use only stack or accumulator, but most used ISAs use registers, so we'll ignore those architectures. }.
    \item Addressing modes - Possible methods to refer to memory or register.
\end{itemize}
There are other specification, however they are not relevant to this thesis. For each instruction and operand there is
specified how they should be encoded into binary (remember, that's what the CPU can understand).
CPU then \textit{implements} some ISA. If two different CPUs implements the same ISA then they should be able to run the same machine code program.
For example, most PC use the \textit{x86} architecture \cite{aps-isa}, althrough the ARM architecture is also seeing use in personal computers, for example the Apple Sillicon is of ARM architecture.

The x86 architecture is so called \textit{Complex Instruction Set Architecture} (CISC).
It contains many instructions that do many things at once, have varying length and takes multiple clock cycles to complete \cite{intel-manual}.
On the other hand the ARM architecture is \textit{Reduced Instruction Set Architecture} (RISC).
The number of instructions is smaller, they are intended to be small building blocks from which complex operations may be created by using many of them. Each instruction in RISC also has same length. Both architectures have their pros and cons, althrough some literature suggest that in modern days the choice of architecture is irrelevant if one is only considering performance and power consuption  \cite{riscvscisc1, riscvscisc2}. Unless specified otherwise the rest of this chapter will be talking about x86. This is because T86 \todo{ref} is loosely based on x86, so it is most relevant for us.

In first chapter we briefly mentioned that machine code programs can instead be written in Assembly language.
Assembly is almost 1:1 mapping to machine code. When showing programs, we will show them in assembly.
In figure \ref{fig:assembly-example2}, we present another example of a program that was compiled from C
to assembly of the x86 architecture.
As seen, instructions have various operands. Most often registers (\texttt{RBP, RSP, EAX}), memory (\texttt{[rbp-4]} is reference to memory at address which
is in register \texttt{rbp} minus $4$), or labels (like \texttt{L2}). Labels are not part of machine code, instead
memory address has to be provided. This is a small part where assembly and machine code differ.
For detailed overview of the x86 instructions see \cite{intel-manual}.

\begin{figure}\label{fig:assembly-example2}
    \begin{lstlisting}
max:
    push    rbp
    mov     rbp, rsp
    mov     QWORD PTR [rbp-24], rdi
    mov     DWORD PTR [rbp-28], esi
    mov     rax, QWORD PTR [rbp-24]
    mov     eax, DWORD PTR [rax]
    mov     DWORD PTR [rbp-4], eax
    mov     DWORD PTR [rbp-8], 1
    jmp     .L2
.L3:
    mov     eax, DWORD PTR [rbp-8]
    cdqe
    lea     rdx, [0+rax*4]
    mov     rax, QWORD PTR [rbp-24]
    add     rax, rdx
    mov     eax, DWORD PTR [rax]
    cmp     DWORD PTR [rbp-4], eax
    cmovge  eax, DWORD PTR [rbp-4]
    mov     DWORD PTR [rbp-4], eax
    add     DWORD PTR [rbp-8], 1
.L2:
    mov     eax, DWORD PTR [rbp-8]
    cmp     eax, DWORD PTR [rbp-28]
    jl      .L3
    mov     eax, DWORD PTR [rbp-4]
    pop     rbp
    ret
    \end{lstlisting}
    \caption{Compiled C program with GCC 9.4 compiler as x86 assembly.}
\end{figure}

\subsection{Registers}

The x86 architecture has a set of general purpose registers.
Some of these are
\begin{itemize}
    \item RAX - Accumulator for operands and results data,
    \item RCX - Counter for string and loop operations,
    \item RSP - Stack pointer,
    \item RBP - Pointer to data on the stack.
\end{itemize}
The names and number of general purpose registers change based on bit mode. 64-bit (also named x86-64) mode has 16 of them, while 32-bit has 8.
Althrough the \texttt{RSP} and \texttt{RBP} are called general purpose they are often only used for pointing at the top of the stack,
resp. to the base of the stack. Stack is special part of program memory, it mostly has LIFO
semantics\footnote{For example the \texttt{mov eax, DWORD PTR [rbp - 4]} does not respect
LIFO semantics, because it reads directly from the stack and not from the top.}
It can be used to store intermediate result, arguments to functions, return address etc.
This register is weird in a sense that it has this very special purpose but is still considered part of
the general purpose registers~\cite{intel-manual}. One might use it for storing calculations, but it would make
rest of the instructions that work with stack behave unexpectedly.

Instruction pointer register (RIP on x86-64, EIP on x86) contains address of the current instruction to be executed. As we mentioned
in Introduction \todo{ref}, programs are executed sequentially from top to bottom, with certain instructions
having the ability to change the control flow. When an instruction get executed, the size of the instruction
will be added to the value in RIP register. This will advance the instruction pointer to the next instruction.
Or, if the instruction changes control flow, the value in instruction pointer will be changed to the destination
of the instruction. The register can also be changed directly.

Another interesting register is the \texttt{EFLAGS} register. The register contains group
of flags, which can alter various behavior of the CPU, or the CPU itself sets them
as result of some instruction. For example the instruction \texttt{cmp} compares its two operands
and if they are the same the \textit{zero} flag in the \texttt{EFLAGS} register will be set.

\subsection{Interrupts}
Interrupt is a special request to the CPU to stop execution of current program and to quickly react to
the reason that caused the request \cite{aps-interrupts}. Example of such event can be keyboard press or error in an program (division by zero).
There are two main categories \cite{intel-manual}
\begin{itemize}
    \item An \textbf{interrupt} is an asynchronous\footnote{Meaning that the interrupt may happen when another instruction is being processed (not on the CPU clock edge).} event that is typically triggered by an Input/Output (IO) device.
    \item An \textbf{exception}\footnote{Unfortunately, this term will become quite overloaded in this thesis.} is a synchronous event that is generated when the processor detects one or more
          predefined conditions when executing an instruction. These are further divided into three classes: faults, traps and aborts.
\end{itemize}

When an interrupt or exception happens, the processor halts execution of current program and switches to specific
interrupt handler. Interrupt handler is just another sequence of instructions that handles the interrupt.
Example of an exception is the \texttt{INT3} instruction.
When this instruction is executed an interrupt is generated. This instruction is specifically meant to be used as a breakpoint.
We can supply code that will be responsible for handling the breakpoint as the interrupt handler.
However, on modern PCs a Operating System (OS) is governing the PC. Alas, we cannot touch the interrupt handler directly.
Instead, an OS is going to have to provide another layer of support for debugging.

Recall the EFLAGS register mentioned in section \ref{X}. There is a special flag called trap flag. When it is set, cpu will issue an interrupt
after every executed instruction. This could be useful if we wanted to inspect execution instruction by instruction.

\todo{Debugging embedded}

\section{Operating system support}
Operating system is a layer between computer components (cpu, memory, input/output devices, \dots) and software. It is responsible for
handling all those resources so programmers do not have to think about it \cite{modern-os, os-concepts}. Managing resources is not only to make writing programs easier, but to make sure that they are
safe from each other. Modern operating system allows to run multiple programs at once (or at least offer the illusion that it can) and they make sure that
one program cannot overwrite data or otherwise interfere with other programs. Normal programs runs in so called \textit{user space}, which has limited capabilites.
Kernel on the other hand runs in \textit{kernel space}. It has full access to hardware of the computer, can use all instructions, can permit or mask interrupts and so on.

However, if programs were limited to user space all the time they would be very limited.
Sometimes, they need to escape the confiment of the OS, for example to read a file or communicate with other processes.
Operating systems provide an interface through which the user space program
can leverage small part of the kernel - system calls. They offer a way of requiring some service from the OS.
This API is often in form of C and C++ functions~\cite{os-concepts}. A part of these functions is a special instruction, like \texttt{SYSCALL} on x86~\cite{intel-manual}, that
switches the mode to kernel space. The kernel has to check if the call is correct, since it will
be executed in kernel space with full access.

The most prelevant operating systems today are Microsoft Windows, Linux and MacOS.
Linux and MacOS systems are somewhat similar, but Windows is very different.

\subsection{Linux}
Linux offers special system call which is very handy for debugging. It is called \texttt{ptrace} \cite{ptrace} - process
trace. It has following signature: \texttt{ptrace(PTRACE\_COMMAND, pid, ...)}. It takes a \texttt{PTRACE\_COMMAND},
which specifies the behaviour of the function (for example \texttt{PTRACE\_SINGLESTEP} for single step), pid of some
process and some other parameters, depending on the \texttt{PTRACE\_COMMAND} that was chosen.
It allows to observe and control the execution of another process, this process will be the debugee.
In the context of \texttt{ptrace}, we will instead use the word tracee, to be consistent with ptrace documentation.

\mintinline{c}{ptrace} has many commands, here are some of the most important:
\begin{itemize}
    \item \texttt{PTRACE\_PEEKTEXT, PTRACE\_PEEKDATA} - Read tracee's memory,
    \item \texttt{PTRACE\_POKETEXT, PTRACE\_POKEDATA} - Write into tracee's memory,
    \item \texttt{PTRACE\_GETREGS} - Read tracee's register values,
    \item \texttt{PTRACE\_SETREGSET} - Modify tracee's register values,
    \item \texttt{PTRACE\_GETSIGINFO} - Retrieve information about the signal that caused tracee to stop,
    \item \texttt{PTRACE\_CONT} - Restart the stopped tracee process,
    \item \texttt{PTRACE\_SINGLESTEP} - Restart the stopped tracee but stop it after executing one instruction.
\end{itemize}

Linux however needs some way of notifying the debugger that the tracee encountered a breakpoint, or that some other
event requiring debugger attention happened. To this end, \textit{signals} are used.
They are in principle similar to CPU interrupts. They are however on the OS level.
A signal is used in UNIX and Linux systems to notify a process that a particular event has occured \cite{os-concepts}.
Signals can be sent to processes. When such process receives a signal, it stops its execution and starts
the execution of a signal handler. There are various signal types. Most signals can have custom signal handler
defined by the process. If no handler is defined then a default one is provided by the OS.
However handlers for \texttt{SIGKILL} and \texttt{SIGSTOP} cannot be changed \cite{signals}.

Reason for rising a signal can be \todo{Tady toho asi bude vic}
\begin{itemize}
    \item CPU Interrupt (Division by zero, Breakpoint hit),
    \item System call (\textit{kill(pid, signal)}).
\end{itemize}

For example, the signal \texttt{SIGTERM} can be send to a process to ask it nicely to exit.
The process can handle this request, for example to save some state before exiting.
It can also however be completely ignored. For this, a signal \texttt{SIGKILL} can be used,
which cannot be handled, ignored or blocked.



\subsection{Windows}
Windows also has built-in support for debugging at the Win32API layer \cite{windows-msdn-debugging-api, windows-press-debugging-api}.
It builds on \textit{debug events} and \textit{debug functions}. Summary of some of the functions that Win32 API offers which all help with debugging:

\begin{itemize}
    \item \mintinline{c}{DebugActiveProcess} - Attaches the debugger to an active process.
    \item \mintinline{c}{DebugBreakProcess} - Causes a breakpoint exception to occur in the specified process.
                                          This passes control of the process to the debugger if there is one.
    \item \mintinline{c}{WaitForDebugEvent} - Waits for new debug events.
    \item \mintinline{c}{ContinueDebugEvent} - Continue the process execution after processing debug event.
    \item \mintinline{c}{OutputDebugString} - Sends a string to the debugger for display.
    \item \mintinline{c}{ReadProcessMemory} and \mintinline{c}{WriteProcessMemory} - Read and modify process virtual address space.
    \item \mintinline{c}{FlushInstructionCache} - Flushes instruction cache of the process.
\end{itemize}

The general structure of Windows debugger can be seen in figure \ref{fig:win32debugger}.
The debugger waits for debug events via function \mintinline{c}{WaitForDebugEvent}.
This function has a timeout parameter, so the debugger can also do other things while it's waiting.
These events are put in a queue, so the debugger will not miss any.

\begin{figure}
    \centering
    \scalebox{0.8}{
    \begin{tikzpicture}
        \draw (-7,0) -- (-7,-11) (0,0) -- (0,-11) (7,0) -- (7,-11);
        \node at (-7,.3) {Debugee};
        \node at (0,.3) {Win32 API};
        \node at (7,.3) {Debugger};
        \draw[<-] (0,-1) -- node[midway,above] {\mintinline{c}{CreateProcess}} (7,-1);
        \draw[<-] (-7,-2) -- node[midway,above] {Create} (0,-2);
        \draw[->] (0,-3) -- node[midway,above] {\mintinline{c}{CreateProcess} returns} (7,-3);
        \draw[<-] (0,-5) -- node[midway,above] {\mintinline{c}{ContinueDebugProcess}} (7,-5);
        \draw[<-] (0,-6) -- node[midway,above] {\mintinline{c}{WaitForDebugEvent}} (7,-6);
        \draw[dashed,->] (-7,-6.5) -- node[midway,above] {Exception} (0,-6.5);
        \draw[->] (0,-7) -- node[midway,above] {\mintinline{c}{WaitForDebugEvent} returns \texttt{true}} (7,-7);
        \draw[dashed, <-] (-7, -8) -- node[above left] {Debugger actions} (7, -8);
        \draw[<-] (0,-9) -- node[midway,above] {\mintinline{c}{ContinueDebugProcess}} (7,-9);
        \draw[<-] (0,-10) -- node[midway,above] {\mintinline{c}{WaitForDebugEvent}} (7,-10);
        \draw[dashed,->] (-7,-10.5) -- node[midway,above] {Exception} (0,-10.5);
    \end{tikzpicture}
    }
    \caption{A sequence diagram for debugger using Windows api. Inspired by \todo{NI-REV 6. lecture}}
    \label{fig:win32debugger}
\end{figure}

The debug events are thoroughly described in subsection \ref{section:Debug Events}. The main point of interest is the exceptions.
By these, we do not mean the standard C++ exceptions but rather Microsoft \textit{Structured Exception Handling}.

\subsubsection*{Debug Events}\label{section:Debug Events}
Debugging events are various incidents in the debuggee that causes the system to notify the debugger \cite{windows-msdn-debug-events}. These are stored in special \mintinline{c}{DEBUG_EVENT} structure, which is received in \texttt{WaitForDebugEvent} call from debugger. This structure contains various information about the event, the internals can be seen on figure \ref{fig:DebugEvent}. These events include loading and unloading a DLL, creating and exiting a process, sending debug strings via the \mintinline{c}{OutputDebugString} and so on. It also includes exceptions, those are probably the most important for us. 
\begin{figure}
\begin{minted}{c}
typedef struct _DEBUG_EVENT {
  DWORD dwDebugEventCode;
  DWORD dwProcessId;
  DWORD dwThreadId;
  union {
    EXCEPTION_DEBUG_INFO      Exception;
    CREATE_THREAD_DEBUG_INFO  CreateThread;
    CREATE_PROCESS_DEBUG_INFO CreateProcessInfo;
    EXIT_THREAD_DEBUG_INFO    ExitThread;
    EXIT_PROCESS_DEBUG_INFO   ExitProcess;
    LOAD_DLL_DEBUG_INFO       LoadDll;
    UNLOAD_DLL_DEBUG_INFO     UnloadDll;
    OUTPUT_DEBUG_STRING_INFO  DebugString;
    RIP_INFO                  RipInfo;
  } u;
} DEBUG_EVENT, *LPDEBUG_EVENT;
\end{minted}
\caption{Structure which contains info about debug event.}
\label{fig:DebugEvent}
\end{figure}

\subsubsection*{Structured Exception Handling}
This feature is specific to Windows only. For example, if division by zero was performed in a program on Linux,
a signal would be sent to the process. Windows don't have signals, instead, it uses Structured Exception Handling \cite{windows-msdn-seh}. 
From now on, we will be using the abbreviation 'SEH'.
An exception is an event that requires execution of code outside the normal flow of control. There are software exceptions,
like throwing an exception explicitly or by OS, and hardware exceptions, like the division by zero we mentioned.
Instruction with opcode \mintinline{c}{0xCC}, which is used for breakpoints, will also raise an exception. SEH unifies both of these things into one.

When an exception is triggered, control is transferred to the system. It saves the state of the thread and some other information.
This information can be used to continue execution from the point where the exception was thrown when it is resolved. It also
contains information about which type of exception was thrown, if execution can continue after handling the exception, address where the
exception occured and some others\footnote{See MSDN documentation \cite{windows-msdn-seh} for full detailed list}.
The system then searches for an exception handler which will handle the exception. The search is performed in this order:

\begin{enumerate}
    \item If the process is debugged the debugger is notified.
    \item If it is not or the debugger does not handle the exception, the frame-based exception handler is to be found\footnote{The handlers are not very important to us, see MSDN documentation if you're interested \cite{windows-msdn-seh}.}
    \item If no frame-based handler can be found, or no handler handles the exception, but the process is being debugged then the debugger gets notified once again.
    \item The system provides default handling, which is to terminate the program via \mintinline{c}{ExitProcess} most of the time.
\end{enumerate}

Here we see that every exception that occurs in the debuggee causes the debugger to be notified. Breakpoints are also caused by an exception, as was briefly mentioned before. There are two possible notifications to the debugger. The first is known as \textit{first-chance} notification \cite{windows-msdn-dbg-exc-handling}. The debugger can (and should) inspect the information about the exception and see if it was a breakpoint or single-step. These only occurs if the process is debugged (it wouldn't happen otherwise) and the debugger should handle them. If it is something else it can ignore the exceptions. When the program is continued via \mintinline{c}{ContinueDebugEvent}\footnote{This function has a special parameter, which is used to tell that the exception was or was not handled.}, the debugger is notified once again if no appropriate exception handler was found for the exception. This is known as \textit{last-chance} notification because if the debugger does not handle the exception the debuggee will be terminated. It gives the user a chance to debug why is his process terminating.

Here are some exceptions that tie into debugging:
\begin{itemize}
    \item \mintinline{c}{STATUS_BREAKPOINT} - Raised when a hardware-defined breakpoint was encountered. This includes the mentioned \mintinline{c}{INT3} instruction.
    \item \mintinline{c}{STATUS_SINGLE_STEP} - Raised when a single step was completed, ie. when instruction was executed and the trap flag is set.
\end{itemize}

\subsubsection*{Tying it all together}
Now we have all necessary building block to build a simple proof of concept Windows debugger. On figure \ref{fig:windows-debugger-mainloop}, you can see a basic idea of a main loop of the debugger. It waits for debug events and branches depending of the type of event. It needs not only handle exceptions, but other events also. For example if the debugee creates a thread that is something the debugger should be aware of. Modern debuggers trace all threads of the program.

\todo{Pridat dalsi figure kde je jak se hanndlujou tyhle blbosti} However, exceptions are the most interesting for us. There, breakpoint and single step handling should be done. On both of these, the debugger should handle the exception itself, so this is the \textit{first chance} notifications. There is also an \mintinline{DBG_CONTROL_C}, which happens on CTRL + C keyboard press. This should terminate the program. The debugger will pass the first chance and catch the last chance exception, so user has a final chance to look at the program state before it exits.

\begin{figure}
    \begin{minted}{c}
void EnterDebugLoop(const LPDEBUG_EVENT DebugEv)
{
   DWORD dwContinueStatus = DBG_CONTINUE; // exception continuation
   for(;;)
   {
      WaitForDebugEvent(DebugEv, INFINITE);
      switch (DebugEv->dwDebugEventCode)
      {
         case EXCEPTION_DEBUG_EVENT:
            // Handle exception debug events
         // Other debug events
      }
   ContinueDebugEvent(DebugEv->dwProcessId,
                      DebugEv->dwThreadId,
                      dwContinueStatus);
   }
}
\end{minted}
\caption{Windows debugger main loop}
\label{fig:windows-debugger-mainloop}
\end{figure}

\section{Compiler support}
\todo{Moved from introduction here for the time being}
When we talked about evolution of programming from machine code to assembly to higher level languages, we haven't
talked about how they are executed. Machine code can be directly executed by processor, as we said, it is a sequence
of binary. Assembly is text, processors don't understand text. But assembly can be mapped to machine code almost 
1:1\footnote{There are some exceptions, like labels. But translating them is not very difficult.}.

However, high level programming languages do not map 1:1 to assembly. Some are close to it, like C, while others
are miles away, like Haskell. But as was said, processors understand only machine code. To this end, programs that
can translate source code into machine code, were created. They are called compilers and the translation process is
called compiling. For example, for the C language one might use the GCC or Clang compilers.
On figure \ref{fig:compiler-structure} can be seen basic structure of a compiler \cite{dragon-book}. 

\tikzstyle{compilerblock} = [rectangle, draw, minimum width=6cm, minimum height=1cm] 
\tikzstyle{tables} = [rectangle, draw, minimum width=4cm, minimum height=1cm] 
\begin{figure}\label{fig:compiler-structure}
    {\centering
    \begin{tikzpicture}
    \node (lexer)[compilerblock]{Lexical analyzer};
    \node (syntax)[compilerblock,below=of lexer]{Syntactic analyzer};
    \node (semantic)[compilerblock,below=of syntax]{Semantic analyzer};
    \node (imc)[compilerblock,below=of semantic]{Intermediate Code Generator};
    \node (gen)[compilerblock,below=of imc]{Code Generator};
    \node (symbol)[tables, left=of semantic]{Symbol table};
    \draw[->] (lexer) -- node[below] {} (syntax);
    \draw[->] (syntax) -- node[below] {} (semantic);
    \draw[->] (semantic) -- node[below] {} (imc);
    \draw[->] (imc) -- node[below] {} (gen);
    \end{tikzpicture} 
    \par}
    \caption{Simplified structure of a compiler. Some parts were left out, like optimizations.}
    \label{fig:compiler_tikz}
\end{figure}

\subsection{Lexical analyzer}
The lexical analyzer groups separate symbols into groups. For example the code
\begin{minted}{c}
foo = bar(1 + 2);
\end{minted}
might be translated into tokens like this
\begin{lstlisting}[stringstyle=\color{black}]
<id:"foo"> <assignment-operator> <id:"bar"> 
<left-bracket> <int-number:1> <plus-operator> 
<int-number:2> <right-bracket> <semicolon>
\end{lstlisting}
The Syntactic analyzer then works with these tokens.

\subsection{Syntantic and semantic analyzer}
Syntactic analysis accepts tokens and processes them into other intermediate representation. This is
most often an abstract syntax tree (abbr. AST, figure \ref{fig:ast}). It also checks that the source code complies to the grammar of the language.
Semantic analysis then checks that the program is semantically consistent. For example that used variable
has been declared before.

\begin{figure}\label{fig:ast}
    \centering
    \begin{tikzpicture}[,shorten >=1pt,node distance=1.8cm,on grid,initial/.style={}]
    \node (assignment) {$=$};
    \node (foo) [below left =of assignment] {id:foo};
    \node (bar) [below right =of assignment] {call:bar};
    \node (plus) [below right=of bar] {$+$};
    \node (one) [below left =of plus] {$1$};
    \node (two) [below right =of plus] {$2$};
    
    \draw[-, above, scale=0.7] 
    (assignment)   edge node[scale=0.7, left, yshift=0.1cm] {lhs}  (foo)
     (assignment)  edge node[scale=0.7, right, yshift=0.1cm] {rhs}  (bar)
     (bar)         edge node[scale=0.7, right, yshift=0.1cm] {expr} (plus)
     (plus)        edge node[scale=0.7, right, yshift=0.1cm] {rhs}  (two)
     (plus)        edge node[scale=0.7, left, yshift=0.1cm] {lhs}  (one);
    \end{tikzpicture}
    \caption{Simplified example of an abstract syntax tree.}
    \label{fig:astgraph}
\end{figure}
 
\subsection{Intermediate code generation}
This part converts AST into some other representation, most commonly called IR\footnote{IR means intermediate representation.
AST is also intermediate representation, but if we use IR we mean this one.}. IR is closer to machine code, to be easily translated,
but retain some properties that makes it easier to work with it. There are many types of IR. One of the most popular compilers, LLVM, uses
single static assignment (SSA) \cite{llvm}. Example of LLVM IR can be found on figure \ref{fig:llvm-ir-example}. Compilers perform most
optimizations on this intermediate representation. 

\begin{figure}\label{fig:llvm-ir-example}
    \begin{minted}{llvm}
        define dso_local i32 @_Z6squarei(i32 %0) {
          %2 = alloca i32, align 4
          store i32 %0, i32* %2, align 4
          %3 = load i32, i32* %2, align 4
          %4 = load i32, i32* %2, align 4
          %5 = mul nsw i32 %3, %4
          ret i32 %5
        }
    \end{minted}
    \caption{Simplified example of LLVM IR.}
\end{figure}

\subsection{Code generation}
Here, IR is translated directly to the target machine code or possibly assembly. Even though IR can seem very similar to assembly,
there are still some things to take care of. For example SSA IR doesn't have registers, it uses unlimited number of variables.
Other architectures might have some other traits that differ it from the IR and they all have to be accounted for when generating code.

\subsection{Modularity of compilers}
The main advantage of using an IR is that there is a common ground for every language. Imagine we write a compiler for the C language.
We need to write all five parts from figure \ref{fig:compiler-structure}. If we later decided that we also want to create a compiler
for Haskell, we just need to write everything up to the IR translation. Once we can translate Haskell into the IR, we can reuse the
previous part of the compiler to compile to machine code! This also works the other way around. If we compiled IR to the machine code
that works with the x86 architecture, and we want to compile to ARM, we just need to create the code generation part for the ARM architecture,
no need to write whole compiler. Also, most of the optimizations are done on the IR level, this also saves a lot of development time.
The parts of the compiler which are dependent on the source language are called \textbf{frontend} (Syntax, Semantic and IR translation), the parts that are dependent on
the target are called \textbf{backend} (Code generation).

This is widely used in practice. The LLVM \cite{llvm} project is a compiler backend. It uses its own IR (as was mentioned on figure \ref{fig:llvm-ir-example}).
It can compile this IR into many targets, including x86, ARM and Spark \todo{Ocitovat}. The \textit{Clang} project is a compiler frontend for C, C++ and Objective-C languages.
It translates these languages to the LLVM IR. Other frontends for LLVM also include \textit{ghc}, which is a Haskell compiler, or \textit{rustc}, which is a Rust compiler.
With LLVM, creating new programming language comes down to parsing it into an AST and transforming that AST into the LLVM IR.

\subsection{Interpreting programs}
Not all languages are compiled. Imagine a program which can evaluate arithmetic expressions, each phone nowadays has a program like this.
We don't have to stop there. Moving this up a notch, we can create a program that reads source code and executes it.
This is what interpreting means. Dynamically typed languages tend to be interpreted~\cite{python, lua, javascript}\todo{Instead of languages, cite some relevant source}, but it is not a rule~\cite{scala}. Since the interpreter is a program, it is another layer of abstraction. This can make
the resulting languages sometimes more abstract then the compiled ones, sometimes at the cost of performance~\cite{jit}.
Interpreters still \textit{compile} the code into some intermediate representation, but it's not compiled down to machine code, instead that IR is run by a program\footnote{Nowadays, interpreters use JIT compilation, which compiles some of the code some of the time into machine code~\cite{jit}}.
